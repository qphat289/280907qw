{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-14T17:32:33.400909Z","iopub.status.busy":"2024-10-14T17:32:33.400217Z","iopub.status.idle":"2024-10-14T17:34:09.490266Z","shell.execute_reply":"2024-10-14T17:34:09.489148Z","shell.execute_reply.started":"2024-10-14T17:32:33.400869Z"},"trusted":true},"outputs":[],"source":["!pip install -q qwen-vl-utils\n","!pip install -q git+https://github.com/huggingface/transformers@21fac7abba2a37fae86106f87fcf9974fd1e3830\n","!pip install -q accelerate\n","!pip -q install sentence-transformers==3.0.1"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!git clone https://github.com/qphat289/finetune_qwenvl2_with_llamafactory.git"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T16:01:36.682880Z","iopub.status.busy":"2024-10-14T16:01:36.682469Z","iopub.status.idle":"2024-10-14T16:01:37.684442Z","shell.execute_reply":"2024-10-14T16:01:37.683373Z","shell.execute_reply.started":"2024-10-14T16:01:36.682841Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/working/finetune_qwenvl2_with_llamafactory\n","/kaggle/working/finetune_qwenvl2_with_llamafactory\n"]}],"source":["%cd /kaggle/working/finetune_qwenvl2_with_llamafactory\n","!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install bitsandbytes\n","!pip install -r requirements.txt\n","!pip install git+https://github.com/huggingface/transformers.git\n","!pip install -e \".[torch, metrics]\"\n","!pip install liger-kernel"]},{"cell_type":"markdown","metadata":{},"source":["# DATA PREPARE"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T17:34:09.492409Z","iopub.status.busy":"2024-10-14T17:34:09.492101Z","iopub.status.idle":"2024-10-14T17:34:28.905848Z","shell.execute_reply":"2024-10-14T17:34:28.904637Z","shell.execute_reply.started":"2024-10-14T17:34:09.492377Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n","  from tqdm.autonotebook import tqdm, trange\n"]}],"source":["from sentence_transformers import SentenceTransformer\n","import torch\n","import torch.nn.functional as F\n","import argparse\n","import numpy as np\n","import json\n","\n","from IPython.display import clear_output as cls\n","from IPython.display import FileLink\n","\n","import time\n","from tqdm.auto import tqdm\n","import json\n","from io import BytesIO\n","import re\n","from torch import nn\n","from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n","from qwen_vl_utils import process_vision_info\n","\n","# default: Load the model on the available device(s)\n","model = Qwen2VLForConditionalGeneration.from_pretrained(\n","    \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",")\n","\n","processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T17:35:20.886422Z","iopub.status.busy":"2024-10-14T17:35:20.885810Z","iopub.status.idle":"2024-10-14T17:35:52.831954Z","shell.execute_reply":"2024-10-14T17:35:52.831079Z","shell.execute_reply.started":"2024-10-14T17:35:20.886383Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Wrong format at id 236182\n","Wrong format at id 320078\n","Wrong format at id 65209\n","Wrong format at id 29140\n","Wrong format at id 200857\n","Wrong format at id 481810\n","Wrong format at id 525152\n","Wrong format at id 332296\n","Wrong format at id 238390\n","Wrong format at id 572985\n","Wrong format at id 11373\n","Wrong format at id 520778\n","Wrong format at id 54899\n","Wrong format at id 119974\n","Wrong format at id 36082\n","Wrong format at id 547487\n","Wrong format at id 286149\n","Wrong format at id 363718\n","Wrong format at id 563178\n","Wrong format at id 66067\n","Wrong format at id 26809\n"]}],"source":["from datasets import load_dataset\n","import pandas as pd\n","import os\n","import torch\n","from PIL import Image\n","\n","ds = load_dataset(\"Vi-VLM/Vista\", \"vi_llava_complex_reasoning\", split='train')\n","root_image = '/kaggle/input/coco-2017-dataset/coco2017/train2017'\n","\n","data = []\n","for i in ds:\n","    try:\n","        data.append({\n","            \"messages\": [\n","                    {\n","                    \"content\": \"<image>\" + i['conversation'][0]['content'],\n","                    \"role\": \"user\"\n","                    },\n","                    {\n","                    \"content\": i['conversation'][1]['content'],\n","                    \"role\": \"assistant\"\n","                    }\n","                ],\n","            \"images\": [\n","                \"data/vista/\" + i['file_name']\n","            ]\n","        }\n","        )\n","            \n","#             'question_id': i['id'],\n","#             'image_id': os.path.join(root_image,i['file_name']),\n","#             'question': i['conversation'][0]['content'],\n","#             'answer': i['conversation'][1]['content']\n","#         }])\n","    except IndexError:\n","        print(f\"Wrong format at id {i['id']}\")\n","        continue"]},{"cell_type":"markdown","metadata":{},"source":["/kaggle/input/coco-2017-dataset/coco2017/train2017/"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T17:35:52.834035Z","iopub.status.busy":"2024-10-14T17:35:52.833727Z","iopub.status.idle":"2024-10-14T17:35:52.840732Z","shell.execute_reply":"2024-10-14T17:35:52.839788Z","shell.execute_reply.started":"2024-10-14T17:35:52.834003Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'messages': [{'content': '<image>Ngoài các thiết bị được nhìn thấy rõ ràng, liệu căn bếp này có đủ chỗ cho những thiết bị nào khác không?',\n","   'role': 'user'},\n","  {'content': 'Bếp rất hẹp và không có đủ chỗ cho các thiết bị khác. Cả hai bên của nhà bếp đều có tủ và thiết bị, và không có không gian trống để đặt thêm thiết bị. Ngoài ra, tủ lạnh và lò nướng đã chiếm phần lớn không gian trên tường, không còn chỗ cho các thiết bị lớn hơn.',\n","   'role': 'assistant'}],\n"," 'images': ['data/vista/000000403013.jpg']}"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["data[2]"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T17:36:50.287567Z","iopub.status.busy":"2024-10-14T17:36:50.287169Z","iopub.status.idle":"2024-10-14T17:36:54.578096Z","shell.execute_reply":"2024-10-14T17:36:54.577060Z","shell.execute_reply.started":"2024-10-14T17:36:50.287529Z"},"trusted":true},"outputs":[],"source":["import json\n","json_file = 'newest_qwenvl2.json'\n","with open(json_file, 'w', encoding='utf-8') as f:\n","            json.dump(data, f, indent=2, ensure_ascii=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Train with LLaMA Factory"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T16:35:25.928681Z","iopub.status.busy":"2024-10-14T16:35:25.927976Z","iopub.status.idle":"2024-10-14T16:35:26.908619Z","shell.execute_reply":"2024-10-14T16:35:26.907513Z","shell.execute_reply.started":"2024-10-14T16:35:25.928639Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["'pwd' is not recognized as an internal or external command,\n","operable program or batch file.\n"]}],"source":["!pwd"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T16:05:24.130796Z","iopub.status.busy":"2024-10-14T16:05:24.130375Z","iopub.status.idle":"2024-10-14T16:05:24.139068Z","shell.execute_reply":"2024-10-14T16:05:24.138086Z","shell.execute_reply.started":"2024-10-14T16:05:24.130758Z"},"trusted":true},"outputs":[],"source":["import json\n","\n","args = dict(\n","  stage=\"sft\",                        # do supervised fine-tuning\n","  do_train=True,\n","  model_name_or_path=\"Qwen/Qwen2-VL-2B-Instruct\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n","  dataset=\"vista_data\",             # use alpaca and identity datasets\n","  template=\"qwen2_vl\",                     # use llama3 prompt template\n","  finetuning_type=\"lora\",                   # use LoRA adapters to save memory\n","  lora_target=\"all\",                     # attach LoRA adapters to all linear layers\n","  output_dir=\"qwen2vl_lora\",                  # the path to save LoRA adapters\n","  per_device_train_batch_size=2,               # the batch size\n","  gradient_accumulation_steps=4,               # the gradient accumulation steps\n","  lr_scheduler_type=\"cosine\",                 # use cosine learning rate scheduler\n","  logging_steps=10,                      # log every 10 steps\n","  warmup_ratio=0.1,                      # use warmup scheduler\n","  save_steps=20,                      # save checkpoint every 1000 steps\n","  learning_rate=5e-5,                     # the learning rate\n","  num_train_epochs=3.0,                    # the epochs of training\n","  max_samples=500,                      # use 500 examples in each dataset\n","  max_grad_norm=1.0,                     # clip gradient norm to 1.0\n","  loraplus_lr_ratio=16.0,                   # use LoRA+ algorithm with lambda=16.0\n","  fp16=True,                         # use float16 mixed precision training\n","  use_liger_kernel=True,                   # use liger kernel for efficient training\n",")"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T16:05:27.559633Z","iopub.status.busy":"2024-10-14T16:05:27.559276Z","iopub.status.idle":"2024-10-14T16:05:27.564545Z","shell.execute_reply":"2024-10-14T16:05:27.563610Z","shell.execute_reply.started":"2024-10-14T16:05:27.559601Z"},"trusted":true},"outputs":[],"source":["json.dump(args, open(\"train_qwen2vl.json\", \"w\", encoding=\"utf-8\"), indent=2)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T16:05:32.909277Z","iopub.status.busy":"2024-10-14T16:05:32.908915Z","iopub.status.idle":"2024-10-14T16:06:04.875080Z","shell.execute_reply":"2024-10-14T16:06:04.873962Z","shell.execute_reply.started":"2024-10-14T16:05:32.909243Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["'llamafactory-cli' is not recognized as an internal or external command,\n","operable program or batch file.\n"]}],"source":["!llamafactory-cli train train_qwen2vl.json"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T16:06:04.878049Z","iopub.status.busy":"2024-10-14T16:06:04.877721Z","iopub.status.idle":"2024-10-14T16:06:04.883566Z","shell.execute_reply":"2024-10-14T16:06:04.882668Z","shell.execute_reply.started":"2024-10-14T16:06:04.878015Z"},"trusted":true},"outputs":[],"source":["args = dict(\n","  model_name_or_path=\"Qwen/Qwen2-VL-2B-Instruct\", # use official non-quantized Llama-3-8B-Instruct model\n","  adapter_name_or_path=\"qwen2vl_lora\",            # load the saved LoRA adapters\n","  template=\"qwen2_vl\",                     # same to the one in training\n","  finetuning_type=\"lora\",                  # same to the one in training\n","  export_dir=\"qwen2vl_2b_instruct_lora_merged\",              # the path to save the merged model\n","  export_size=2,                       # the file shard size (in GB) of the merged model\n","  export_device=\"cuda\",                    # the device used in export, can be chosen from `cpu` and `cuda`\n","  #export_hub_model_id=\"your_id/your_model\",         # the Hugging Face hub ID to upload model\n",")\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T16:06:04.885039Z","iopub.status.busy":"2024-10-14T16:06:04.884680Z","iopub.status.idle":"2024-10-14T16:06:04.894324Z","shell.execute_reply":"2024-10-14T16:06:04.893491Z","shell.execute_reply.started":"2024-10-14T16:06:04.884993Z"},"trusted":true},"outputs":[],"source":["json.dump(args, open(\"merge_qwen2vl.json\", \"w\", encoding=\"utf-8\"), indent=2)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T16:06:04.896178Z","iopub.status.busy":"2024-10-14T16:06:04.895852Z","iopub.status.idle":"2024-10-14T16:06:23.041810Z","shell.execute_reply":"2024-10-14T16:06:23.040836Z","shell.execute_reply.started":"2024-10-14T16:06:04.896146Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[INFO|configuration_utils.py:675] 2024-10-14 16:06:19,190 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/config.json\n","[INFO|configuration_utils.py:742] 2024-10-14 16:06:19,193 >> Model config Qwen2VLConfig {\n","  \"_name_or_path\": \"Qwen/Qwen2-VL-2B-Instruct\",\n","  \"architectures\": [\n","    \"Qwen2VLForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 151643,\n","  \"eos_token_id\": 151645,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 1536,\n","  \"image_token_id\": 151655,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 8960,\n","  \"max_position_embeddings\": 32768,\n","  \"max_window_layers\": 28,\n","  \"model_type\": \"qwen2_vl\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 28,\n","  \"num_key_value_heads\": 2,\n","  \"rms_norm_eps\": 1e-06,\n","  \"rope_scaling\": {\n","    \"mrope_section\": [\n","      16,\n","      24,\n","      24\n","    ],\n","    \"rope_type\": \"default\",\n","    \"type\": \"default\"\n","  },\n","  \"rope_theta\": 1000000.0,\n","  \"sliding_window\": 32768,\n","  \"tie_word_embeddings\": true,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.45.2\",\n","  \"use_cache\": true,\n","  \"use_sliding_window\": false,\n","  \"video_token_id\": 151656,\n","  \"vision_config\": {\n","    \"hidden_size\": 1536,\n","    \"in_chans\": 3,\n","    \"model_type\": \"qwen2_vl\",\n","    \"spatial_patch_size\": 14\n","  },\n","  \"vision_end_token_id\": 151653,\n","  \"vision_start_token_id\": 151652,\n","  \"vision_token_id\": 151654,\n","  \"vocab_size\": 151936\n","}\n","\n","[INFO|tokenization_utils_base.py:2206] 2024-10-14 16:06:19,241 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/vocab.json\n","[INFO|tokenization_utils_base.py:2206] 2024-10-14 16:06:19,241 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/merges.txt\n","[INFO|tokenization_utils_base.py:2206] 2024-10-14 16:06:19,241 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/tokenizer.json\n","[INFO|tokenization_utils_base.py:2206] 2024-10-14 16:06:19,241 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2206] 2024-10-14 16:06:19,241 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:2206] 2024-10-14 16:06:19,241 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2470] 2024-10-14 16:06:19,636 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","[INFO|image_processing_base.py:375] 2024-10-14 16:06:19,791 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/preprocessor_config.json\n","[INFO|image_processing_base.py:375] 2024-10-14 16:06:19,846 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/preprocessor_config.json\n","[INFO|image_processing_base.py:429] 2024-10-14 16:06:19,846 >> Image processor Qwen2VLImageProcessor {\n","  \"do_convert_rgb\": true,\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.48145466,\n","    0.4578275,\n","    0.40821073\n","  ],\n","  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n","  \"image_std\": [\n","    0.26862954,\n","    0.26130258,\n","    0.27577711\n","  ],\n","  \"max_pixels\": 12845056,\n","  \"merge_size\": 2,\n","  \"min_pixels\": 3136,\n","  \"patch_size\": 14,\n","  \"processor_class\": \"Qwen2VLProcessor\",\n","  \"resample\": 3,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"max_pixels\": 12845056,\n","    \"min_pixels\": 3136\n","  },\n","  \"temporal_patch_size\": 2\n","}\n","\n","[INFO|tokenization_utils_base.py:2206] 2024-10-14 16:06:19,888 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/vocab.json\n","[INFO|tokenization_utils_base.py:2206] 2024-10-14 16:06:19,888 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/merges.txt\n","[INFO|tokenization_utils_base.py:2206] 2024-10-14 16:06:19,888 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/tokenizer.json\n","[INFO|tokenization_utils_base.py:2206] 2024-10-14 16:06:19,888 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2206] 2024-10-14 16:06:19,888 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:2206] 2024-10-14 16:06:19,888 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/aca78372505e6cb469c4fa6a35c60265b00ff5a4/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2470] 2024-10-14 16:06:20,278 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","[INFO|processing_utils.py:744] 2024-10-14 16:06:21,056 >> Processor Qwen2VLProcessor:\n","- image_processor: Qwen2VLImageProcessor {\n","  \"do_convert_rgb\": true,\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.48145466,\n","    0.4578275,\n","    0.40821073\n","  ],\n","  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n","  \"image_std\": [\n","    0.26862954,\n","    0.26130258,\n","    0.27577711\n","  ],\n","  \"max_pixels\": 12845056,\n","  \"merge_size\": 2,\n","  \"min_pixels\": 3136,\n","  \"patch_size\": 14,\n","  \"processor_class\": \"Qwen2VLProcessor\",\n","  \"resample\": 3,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"max_pixels\": 12845056,\n","    \"min_pixels\": 3136\n","  },\n","  \"temporal_patch_size\": 2\n","}\n","\n","- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2-VL-2B-Instruct', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n","\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}\n","\n","{\n","  \"processor_class\": \"Qwen2VLProcessor\"\n","}\n","\n","Traceback (most recent call last):\n","  File \"/opt/conda/bin/llamafactory-cli\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/kaggle/working/finetune_qwenvl2_with_llamafactory/src/llamafactory/cli.py\", line 87, in main\n","    export_model()\n","  File \"/kaggle/working/finetune_qwenvl2_with_llamafactory/src/llamafactory/train/tuner.py\", line 75, in export_model\n","    get_template_and_fix_tokenizer(tokenizer, data_args)\n","  File \"/kaggle/working/finetune_qwenvl2_with_llamafactory/src/llamafactory/data/template.py\", line 361, in get_template_and_fix_tokenizer\n","    require_version(\"accelerate>=0.34.0\", \"To fix: pip install accelerate>=0.34.0\")\n","  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/versions.py\", line 111, in require_version\n","    _compare_versions(op, got_ver, want_ver, requirement, pkg, hint)\n","  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/versions.py\", line 44, in _compare_versions\n","    raise ImportError(\n","ImportError: accelerate>=0.34.0 is required for a normal functioning of this module, but found accelerate==0.33.0.\n","To fix: pip install accelerate>=0.34.0\n"]}],"source":["!llamafactory-cli export merge_qwen2vl.json"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"qwen","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
